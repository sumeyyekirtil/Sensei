# chatbot_project.py (rag sistem ve arayÃ¼z entegre)

import os
import streamlit as st
from dotenv import load_dotenv

# LangChain BileÅŸenleri (Stabil ve Yeni YapÄ±)
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate 
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# --- YAPILANDIRMA ---
load_dotenv()
CHROMA_DB_PATH = "./chroma_db" 

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY") 
if not GEMINI_API_KEY:
    st.error("GEMINI_API_KEY bulunamadÄ±. LÃ¼tfen .env dosyanÄ±zÄ± kontrol edin.")
    st.stop()
# --------------------


# --- 1. FONKSÄ°YON: RAG Zincirini Kurma (MANUEL ZÄ°NCÄ°R) ---
@st.cache_resource
def setup_rag_chain():
    """RAG sistemini stabil LangChain bileÅŸenleriyle kurar."""
    try:

        # Embeddings Modelini TanÄ±mla (Veri hazÄ±rlama dosyasÄ± ile AYNI OLMALI!)
        embeddings_model = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001", # BU SATIRI DÃœZELTÄ°N!
        google_api_key=GEMINI_API_KEY
        )
          
        # VektÃ¶r Deposu YÃ¼kleniyor
        vector_store = Chroma(
            persist_directory=CHROMA_DB_PATH, 
            embedding_function=embeddings_model
        )

        # Geri Ã‡ekici (Retriever) TanÄ±mlama
        # k=2, daha az ve daha alakalÄ± belge Ã§ekerek LLM'in kafasÄ±nÄ± karÄ±ÅŸtÄ±rmasÄ±nÄ± engeller
        retriever = vector_store.as_retriever(search_kwargs={"k": 3})

        # LLM TanÄ±mlama (API ile stabil Ã§alÄ±ÅŸan model)
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash", # En stabil model adÄ± seÃ§ildi
            temperature=0.4, 
            google_api_key=GEMINI_API_KEY
        )
        
        
        # 1. ADIM: PROMPT ÅABLONUNU OLUÅTURMA
        system_prompt = (
            "Sen bir film Ã¶neri asistanÄ±sÄ±n. YalnÄ±zca saÄŸlanan 'BaÄŸlam' iÃ§erisindeki filmlerle ilgili yanÄ±t ver."
            "YanÄ±tÄ±nÄ± nazik ve kÄ±sa tut, film baÅŸlÄ±klarÄ±nÄ± vurgula."
            "EÄŸer yanÄ±tÄ± BAÄLAM'da bulamÄ±yorsan, 'ÃœzgÃ¼nÃ¼m, aradÄ±ÄŸÄ±n filmi veya bilgiyi veritabanÄ±mda bulamadÄ±m.' de."
            "\n\nBaÄŸlam: {context}"
        )
        
        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
                ("human", "{question}"),
            ]
        )

        # 2. ADIM: MANUEL RAG ZÄ°NCÄ°RÄ°NÄ° OLUÅTURMA
        def format_docs(docs):
            # Geri Ã§ekilen belgeleri tek bir string'de birleÅŸtirir
            return "\n\n".join(doc.page_content for doc in docs)

        rag_chain = (
            # 1. Retriever'Ä± Ã§alÄ±ÅŸtÄ±rÄ±p sonucu 'context'e atar
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            # 2. Prompt'u LLM'e gÃ¶nderir
            | prompt
            | llm
            # 3. YanÄ±tÄ± dÃ¼z metne Ã§evirir
            | StrOutputParser()
        )
        
        return rag_chain

    except Exception as e:
        # GerÃ§ek hatayÄ± gÃ¶steren hata ayÄ±klama mesajÄ± korundu
        st.error(f"RAG Zinciri kurulurken bir hata oluÅŸtu: {e}")
        st.stop()


# --- 2. FONKSÄ°YON: Streamlit ArayÃ¼zÃ¼ ---
def main():
    """Ana Streamlit arayÃ¼z fonksiyonu"""
    st.set_page_config(page_title="Film Ã–neri AsistanÄ± (RAG)", layout="centered")
    st.title("ğŸ¬ Filmler HakkÄ±nda Sohbet AsistanÄ±")
    st.caption("VeritabanÄ±mÄ±zdaki filmler hakkÄ±nda sorular sorun.")

    qa_chain = setup_rag_chain() 

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("Hangi film tÃ¼rlerini seversin veya ne Ã¶nerebilirim?"):
        
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤– Sensei dÃ¼ÅŸÃ¼nÃ¼yor..."):
                answer = ""
                try:
                    # Zinciri sorgulama
                    answer = qa_chain.invoke(prompt) 
                    
                    st.markdown(answer)
                except Exception as e:
                    # GerÃ§ek hatayÄ± gÃ¶steren hata ayÄ±klama mesajÄ±
                    st.error(f"Sorgulama Zincirinde Hata OluÅŸtu: {e}")
                    answer = "ÃœzgÃ¼nÃ¼m, ÅŸu anda bir hata oluÅŸtu veya aradÄ±ÄŸÄ±n bilgiyi veritabanÄ±mda bulamadÄ±m.^_^"
                    st.markdown(answer)

        st.session_state.messages.append({"role": "assistant", "content": answer})

if __name__ == "__main__":
    main()